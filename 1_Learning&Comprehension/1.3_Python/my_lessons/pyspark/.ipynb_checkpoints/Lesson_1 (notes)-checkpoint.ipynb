{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716c7e7-c585-45cb-8c37-d21d26f34475",
   "metadata": {},
   "source": [
    "# History - Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b099c75-2f0a-4642-8d48-8beebf495430",
   "metadata": {},
   "source": [
    "- **2009**: started as a project at UC Berkley AMPLab.\n",
    "- **2010**: open sources (BSD license)\n",
    "- **2013**: Spark became an Apache project.\n",
    "- **2014**: used by Databricks to sort large_scale datasets and set a world record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a6e5c-7d50-4662-be99-5a5c60217991",
   "metadata": {},
   "source": [
    "It is an open-source data processing engine to store and process data in real-time across various clusters of computers using simple programming constructs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1f713-f120-4b6e-b46d-12dda65fee4e",
   "metadata": {},
   "source": [
    "## Hadoop v/s Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0864a19-8658-48f0-906b-5e8fc143cf5b",
   "metadata": {},
   "source": [
    "- Processing data using MapReduce in Hadoop is slow (batch-oriented). Spark processes data 100 times faster than MapReduce as it is done in-memory.\n",
    "\n",
    "- Hadoop performs batch processing whereas Spark can perform both batch processing and real-time processing of data.\n",
    "\n",
    "- Hadoop is written in Java, has more lines of code. Spark is written in Scala and has fewer lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa703e9-47a3-46b1-b6d9-57fd3001b237",
   "metadata": {},
   "source": [
    "## Spark features\n",
    "\n",
    "- Spark contains RDDs (resilient distributed datasets). Think of it as one dataset stored across multiple computers.\n",
    "\n",
    "- In-memory computing. Lazy evaluation. Data is stored in RAM so it can be accessed quickly.\n",
    "\n",
    "- Fault tolerance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ded7c4-5389-48a2-961e-4c110a0da571",
   "metadata": {},
   "source": [
    "## Spark components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b13ba-6c62-4fb5-a969-2032036a672e",
   "metadata": {},
   "source": [
    "<b>Spark core</b> (contains RDDs, core engine) + <b>Spark SQL</b> (for structured data, has dataframes) + <b>Spark streaming</b> (works with data which is being constantly generated) + <b>Spark MLlib</b> (contains libraries for ML development) + <b>Spark GraphX</b> (data with a network structure)\n",
    "\n",
    "a) Spark core is the base engine for large-scale parallel and distributed data processing. Responsible for memory management, fault recovery, scheduling & distributing & monitoring jobs on a cluster, interacting with storage systems. Spark does not have its own storage. The storage could be HDFS, Hbase, any RDBMS etc. <br><br>\n",
    "b) Spark SQL is the framework component used for structured and semi-structured data processing. <br><br>\n",
    "c) Spark streaming is a lightweight API that allows developers to perfrom batch processing and real-time streaming of data with ease. Provides secure, reliable and fast processing of live data streams.<br><br>\n",
    "d) MLlib is a low level ML library that is simple to use, scalable and compatible with various programming languages.<br><br>\n",
    "e) GraphX is Sparks own Graph Computation Engine and data score. Provides a uniform tool for ETL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eab89-a300-47b6-bbb9-78977017340e",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a78ca-db20-4d4e-bc4a-1e141181e264",
   "metadata": {},
   "source": [
    "Apache Spark uses a master slave architecture that consists of a driver, that runs on a master node, and multiple executors which run across the worker nodes in the cluster.\n",
    "\n",
    "https://www.simplilearn.com/tutorials/apache-spark-tutorial/apache-spark-architecture\n",
    "\n",
    "- Master node has a driver program. \n",
    "\n",
    "- The Spark code behaves as a driver program and creates a ```SparkContext```, which is a gateway to all the Spark functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8ed51-b7f0-404d-8926-5b1ce38021f9",
   "metadata": {},
   "source": [
    "## Spark cluster managers\n",
    "\n",
    "Options:<br>\n",
    "a) Apache Spark standalone mode.\n",
    "b) Apache Mesos\n",
    "c) Hadoop Yarn\n",
    "d) Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c36fb-1ef0-49b3-a63c-027353a18b79",
   "metadata": {},
   "source": [
    "## Resilient distributed datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26c504-688d-4cd7-9c63-b8b5753a50cb",
   "metadata": {},
   "source": [
    "Spark core is embedded with RDDs, an immutable fault tolerant, distributed collection of objects that can be operated on in parallel. \n",
    "\n",
    "RDD -> Transformation + Action\n",
    "\n",
    "\n",
    "- **Transformation**: Operations such as map, filter, join union that are performed on an RDD that yields a new RDD with the result.  \n",
    "- **Action**: Operations such as reduce, first, count that return a value after running a computation on an RDD.\n",
    "\n",
    "Transformations only create an execution logic - RDDs. They do not evaluate to anything. Only when you take an action, will the execution gets triggered. \n",
    "\n",
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. \n",
    "\n",
    "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcd928-60c0-4fe6-8d08-583c3891e90a",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "\n",
    "PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities, using PySpark we can run applications parallelly on the distributed cluster (multiple nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a10c4-cc07-4de0-b746-e270295a352f",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b5833-236d-471c-bc76-b319de57572a",
   "metadata": {},
   "source": [
    "In order to create an RDD, first, you need to create a ```SparkSession``` which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c806746-ce21-469e-840e-db462bc6ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af007332-1dc0-47c7-94a7-b1aec5a36038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "# If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn or mesos depends \n",
    "# on your cluster setup.\n",
    "# Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD,\n",
    "# DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "# Used to set your application name.\n",
    "# This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[1]\").appName('PySpark-learn').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd4a470-5fc2-442f-9ddb-559955921774",
   "metadata": {},
   "source": [
    "You can also create a new SparkSession using newSession() method. This uses the same app name, master as the existing session. Underlying SparkContext will be the same for both sessions as you can have only one context per PySpark application.\n",
    "\n",
    "```python\n",
    "# Create new SparkSession\n",
    "spark2 = SparkSession.newSession\n",
    "print(spark2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b59f9-b233-409a-9cad-bb67d1b48451",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
